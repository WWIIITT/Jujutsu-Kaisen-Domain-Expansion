{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e150df3-99e7-4e7c-92a9-2e1dea5e4edf",
   "metadata": {},
   "source": [
    "Check the Domain Expansion hand sign: https://www.wikihow.com/Domain-Expansion-Hand-Sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0958ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015de4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60751dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f770439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f68661-6d72-427d-89ef-57cce036db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Starting Jujutsu Kaisen Domain Detection System...\n",
      "‚úÖ Loaded model from jujutsu_model.pkl\n",
      "‚úÖ Loaded domain image: infinite_void.jpg\n",
      "‚úÖ Loaded domain image: chimera_shadow.jpg\n",
      "‚úÖ Loaded domain image: malevolent_shrine.jpg\n",
      "‚úÖ Jujutsu Kaisen Domain Expansion recognition system initialized\n",
      "\n",
      "==================================================\n",
      "üî• JUJUTSU KAISEN DOMAIN DETECTION üî•\n",
      "==================================================\n",
      "1: üìπ Record Gesture Videos\n",
      "2: üìä Train Model from Videos\n",
      "3: üéØ Start Domain Detection\n",
      "4: üö™ Exit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select option (1-4):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== DOMAIN EXPANSION DETECTION ==\n",
      "üî• Starting real-time detection\n",
      "üî• DOMAIN DETECTED: Malevolent Shrine (Sukuna) (confidence: 0.81)\n",
      "üî• DOMAIN DETECTED: Malevolent Shrine (Sukuna) (confidence: 0.87)\n",
      "üî• DOMAIN DETECTED: Infinite Void (Gojo Satoru) (confidence: 0.81)\n",
      "üî• DOMAIN DETECTED: Infinite Void (Gojo Satoru) (confidence: 1.00)\n",
      "\n",
      "==================================================\n",
      "üî• JUJUTSU KAISEN DOMAIN DETECTION üî•\n",
      "==================================================\n",
      "1: üìπ Record Gesture Videos\n",
      "2: üìä Train Model from Videos\n",
      "3: üéØ Start Domain Detection\n",
      "4: üö™ Exit\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select option (1-4):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Exiting...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import pygame\n",
    "import time\n",
    "import threading\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from pathlib import Path\n",
    "\n",
    "class JujutsuKaisenGestureRecognition:\n",
    "    def __init__(self):\n",
    "        # Initialize MediaPipe Hands\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        \n",
    "        # Domain definitions \n",
    "        self.domain_signs = {\n",
    "            \"infinite_void\": \"Infinite Void (Gojo Satoru)\",\n",
    "            \"chimera_shadow\": \"Chimera Shadow (Megumi Fushiguro)\",\n",
    "            \"malevolent_shrine\": \"Malevolent Shrine (Sukuna)\"\n",
    "        }\n",
    "        \n",
    "        # Paths for data storage\n",
    "        self.data_dir = \"jujutsu_data\"\n",
    "        self.model_path = \"jujutsu_model.pkl\"\n",
    "        self.sound_dir = \"sounds\"\n",
    "        self.image_dir = \"images\"\n",
    "        self.video_dir = \"gesture_videos\"\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        for directory in [self.data_dir, self.sound_dir, self.image_dir, self.video_dir]:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "        \n",
    "        # Initialize pygame for sound\n",
    "        try:\n",
    "            pygame.mixer.init()\n",
    "            self.sounds_available = True\n",
    "        except:\n",
    "            print(\"Sound functionality not available. Continuing without sound effects.\")\n",
    "            self.sounds_available = False\n",
    "        \n",
    "        # Load or create model\n",
    "        self.model = None\n",
    "        if os.path.exists(self.model_path):\n",
    "            self.load_model()\n",
    "        \n",
    "        # Variables for video recording\n",
    "        self.recording = False\n",
    "        self.video_writer = None\n",
    "        self.current_video_path = \"\"\n",
    "        \n",
    "        # Enhanced detection variables\n",
    "        self.last_prediction = None\n",
    "        self.prediction_confidence = 0\n",
    "        self.prediction_time = 0\n",
    "        self.detection_cooldown = 2.0\n",
    "        self.detection_window = 3.0\n",
    "        self.min_detection_time = 1.5\n",
    "        \n",
    "        # For improved prediction smoothing and validation\n",
    "        self.gesture_buffer = []\n",
    "        self.gesture_buffer_size = 10\n",
    "        self.confidence_threshold = 0.8\n",
    "        self.consistency_threshold = 0.7\n",
    "        \n",
    "        # Domain preview images\n",
    "        self.domain_images = {}\n",
    "        self.load_domain_images()\n",
    "        \n",
    "        # Detection state tracking\n",
    "        self.gesture_start_time = 0\n",
    "        self.stable_gesture = None\n",
    "        self.gesture_confirmed = False\n",
    "        \n",
    "        print(\"‚úÖ Jujutsu Kaisen Domain Expansion recognition system initialized\")\n",
    "\n",
    "    def load_domain_images(self):\n",
    "        \"\"\"Load domain expansion character images from the images directory\"\"\"\n",
    "        # Create character images for each domain\n",
    "        character_data = {\n",
    "            \"infinite_void\": {\n",
    "                \"name\": \"Gojo Satoru\",\n",
    "                \"color\": (255, 200, 150),  # Light blue/white\n",
    "                \"bg_color\": (200, 100, 50)\n",
    "            },\n",
    "            \"chimera_shadow\": {\n",
    "                \"name\": \"Megumi Fushiguro\", \n",
    "                \"color\": (150, 100, 200),  # Purple\n",
    "                \"bg_color\": (100, 50, 100)\n",
    "            },\n",
    "            \"malevolent_shrine\": {\n",
    "                \"name\": \"Sukuna\",\n",
    "                \"color\": (100, 100, 255),  # Red\n",
    "                \"bg_color\": (50, 50, 200)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for domain, data in character_data.items():\n",
    "            img_path = os.path.join(self.image_dir, f\"{domain}.jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        # Resize to 200x200 for preview\n",
    "                        img = cv2.resize(img, (200, 200))\n",
    "                        self.domain_images[domain] = img\n",
    "                        print(f\"‚úÖ Loaded domain image: {domain}.jpg\")\n",
    "                    else:\n",
    "                        self.create_character_image(domain, data)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading {img_path}: {e}\")\n",
    "                    self.create_character_image(domain, data)\n",
    "            else:\n",
    "                self.create_character_image(domain, data)\n",
    "\n",
    "    def create_character_image(self, domain, data):\n",
    "        \"\"\"Create a stylized character image for domain preview\"\"\"\n",
    "        # Create 200x200 image\n",
    "        img = np.ones((200, 200, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Create gradient background\n",
    "        for y in range(200):\n",
    "            for x in range(200):\n",
    "                # Radial gradient from center\n",
    "                center_x, center_y = 100, 100\n",
    "                distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "                fade = max(0, 1 - distance / 140)\n",
    "                \n",
    "                color = np.array(data[\"bg_color\"]) * fade + np.array([20, 20, 20]) * (1 - fade)\n",
    "                img[y, x] = color.astype(np.uint8)\n",
    "        \n",
    "        # Add character silhouette/face outline\n",
    "        if domain == \"infinite_void\":\n",
    "            # Gojo's distinctive features - blindfold/eyes\n",
    "            cv2.ellipse(img, (100, 80), (40, 25), 0, 0, 360, (255, 255, 255), -1)\n",
    "            cv2.ellipse(img, (85, 75), (8, 6), 0, 0, 360, (0, 200, 255), -1)\n",
    "            cv2.ellipse(img, (115, 75), (8, 6), 0, 0, 360, (0, 200, 255), -1)\n",
    "            \n",
    "        elif domain == \"chimera_shadow\":\n",
    "            # Megumi's spiky hair silhouette\n",
    "            points = np.array([[100, 40], [85, 60], [70, 50], [90, 80], [100, 70], \n",
    "                              [110, 80], [130, 50], [115, 60], [100, 40]], np.int32)\n",
    "            cv2.fillPoly(img, [points], (80, 80, 120))\n",
    "            \n",
    "        elif domain == \"malevolent_shrine\":\n",
    "            # Sukuna's markings\n",
    "            cv2.line(img, (80, 60), (120, 60), (255, 100, 100), 3)\n",
    "            cv2.line(img, (80, 80), (120, 80), (255, 100, 100), 3)\n",
    "            cv2.line(img, (85, 100), (115, 100), (255, 100, 100), 3)\n",
    "        \n",
    "        # Add domain name\n",
    "        cv2.putText(img, data[\"name\"], (30, 180), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        self.domain_images[domain] = img\n",
    "        print(f\"‚úÖ Created character image for {domain}\")\n",
    "\n",
    "    def extract_hand_features(self, frame):\n",
    "        \"\"\"Extract features from hand landmarks using MediaPipe Hands\"\"\"\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(image_rgb)\n",
    "        \n",
    "        features = np.zeros(84)\n",
    "        hands_detected = False\n",
    "        hand_count = 0\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            hands_detected = True\n",
    "            hand_count = len(results.multi_hand_landmarks)\n",
    "            \n",
    "            # Draw hand landmarks\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    self.mp_hands.HAND_CONNECTIONS,\n",
    "                    self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                    self.mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2)\n",
    "                )\n",
    "            \n",
    "            # Extract features from up to 2 hands\n",
    "            for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks[:2]):\n",
    "                for i, landmark in enumerate(hand_landmarks.landmark):\n",
    "                    base_idx = hand_idx * 42\n",
    "                    features[base_idx + i*2] = landmark.x\n",
    "                    features[base_idx + i*2 + 1] = landmark.y\n",
    "        \n",
    "        return features, frame, hands_detected, hand_count\n",
    "\n",
    "    def update_gesture_buffer(self, prediction, confidence):\n",
    "        \"\"\"Update gesture buffer and check for consistent detection\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        self.gesture_buffer.append({\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'timestamp': current_time\n",
    "        })\n",
    "        \n",
    "        if len(self.gesture_buffer) > self.gesture_buffer_size:\n",
    "            self.gesture_buffer.pop(0)\n",
    "        \n",
    "        self.gesture_buffer = [\n",
    "            entry for entry in self.gesture_buffer \n",
    "            if current_time - entry['timestamp'] < self.detection_window\n",
    "        ]\n",
    "        \n",
    "        if len(self.gesture_buffer) >= 5:\n",
    "            domain_counts = {}\n",
    "            total_confidence = {}\n",
    "            \n",
    "            for entry in self.gesture_buffer:\n",
    "                domain = entry['prediction']\n",
    "                if domain not in domain_counts:\n",
    "                    domain_counts[domain] = 0\n",
    "                    total_confidence[domain] = 0\n",
    "                domain_counts[domain] += 1\n",
    "                total_confidence[domain] += entry['confidence']\n",
    "            \n",
    "            max_count = max(domain_counts.values())\n",
    "            total_predictions = len(self.gesture_buffer)\n",
    "            consistency_ratio = max_count / total_predictions\n",
    "            \n",
    "            if consistency_ratio >= self.consistency_threshold:\n",
    "                dominant_domain = max(domain_counts.keys(), key=lambda k: domain_counts[k])\n",
    "                avg_confidence = total_confidence[dominant_domain] / domain_counts[dominant_domain]\n",
    "                \n",
    "                if dominant_domain != self.stable_gesture:\n",
    "                    self.stable_gesture = dominant_domain\n",
    "                    self.gesture_start_time = current_time\n",
    "                    self.gesture_confirmed = False\n",
    "                \n",
    "                elif current_time - self.gesture_start_time >= self.min_detection_time:\n",
    "                    if not self.gesture_confirmed and avg_confidence >= self.confidence_threshold:\n",
    "                        self.gesture_confirmed = True\n",
    "                        return dominant_domain, avg_confidence\n",
    "        \n",
    "        return None, 0\n",
    "\n",
    "    def show_domain_preview(self, frame, domain, confidence):\n",
    "        \"\"\"Show character preview in upper right corner like in the image\"\"\"\n",
    "        if domain in self.domain_images:\n",
    "            h, w = frame.shape[:2]\n",
    "            preview_size = 150\n",
    "            \n",
    "            # Position in top-right corner with margin\n",
    "            start_x = w - preview_size - 20\n",
    "            start_y = 30\n",
    "            \n",
    "            # Get and resize domain image\n",
    "            domain_img = self.domain_images[domain].copy()\n",
    "            preview_img = cv2.resize(domain_img, (preview_size, preview_size))\n",
    "            \n",
    "            # Add green border (like in your image)\n",
    "            border_thickness = 3\n",
    "            cv2.rectangle(frame, \n",
    "                         (start_x - border_thickness, start_y - border_thickness),\n",
    "                         (start_x + preview_size + border_thickness, start_y + preview_size + border_thickness),\n",
    "                         (0, 255, 0), border_thickness)\n",
    "            \n",
    "            # Place the character image\n",
    "            frame[start_y:start_y+preview_size, start_x:start_x+preview_size] = preview_img\n",
    "            \n",
    "            # Add confidence text below image (green text like in your image)\n",
    "            conf_text = f\"Confidence: {confidence:.2f}\"\n",
    "            cv2.putText(frame, conf_text, \n",
    "                       (start_x, start_y + preview_size + 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    def record_gesture_video(self):\n",
    "        \"\"\"Record gesture videos for training\"\"\"\n",
    "        print(\"\\n== VIDEO RECORDING MODE ==\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"‚ùå Error: Could not open camera.\")\n",
    "            return\n",
    "            \n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "        \n",
    "        print(f\"‚úÖ Camera initialized (FPS: {fps})\")\n",
    "        print(\"Controls: 'R' = Record, 'Q' = Quit\")\n",
    "        \n",
    "        cv2.namedWindow(\"Gesture Video Recorder\", cv2.WINDOW_NORMAL)\n",
    "        video_count = 1\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            try:\n",
    "                features, processed_frame, hands_detected, hand_count = self.extract_hand_features(frame)\n",
    "            except:\n",
    "                processed_frame = frame.copy()\n",
    "                hands_detected = False\n",
    "                hand_count = 0\n",
    "            \n",
    "            # Display recording status\n",
    "            if self.recording:\n",
    "                cv2.putText(processed_frame, f\"üî¥ RECORDING - Video {video_count}\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "                if hasattr(self, 'recording_start_time'):\n",
    "                    elapsed = time.time() - self.recording_start_time\n",
    "                    cv2.putText(processed_frame, f\"Time: {elapsed:.1f}s\", \n",
    "                               (450, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "                \n",
    "                if self.video_writer:\n",
    "                    self.video_writer.write(processed_frame)\n",
    "            else:\n",
    "                cv2.putText(processed_frame, \"Press 'R' to start recording\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            hand_status = f\"üëã Hands: {hand_count}\" if hands_detected else \"‚ùå No Hands\"\n",
    "            color = (0, 255, 0) if hands_detected else (0, 0, 255)\n",
    "            cv2.putText(processed_frame, hand_status, (10, 70), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            \n",
    "            cv2.imshow(\"Gesture Video Recorder\", processed_frame)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('r'):\n",
    "                if self.recording:\n",
    "                    self.recording = False\n",
    "                    if self.video_writer:\n",
    "                        self.video_writer.release()\n",
    "                        self.video_writer = None\n",
    "                    print(f\"‚úÖ Video saved: {self.current_video_path}\")\n",
    "                    video_count += 1\n",
    "                else:\n",
    "                    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    video_filename = f\"gesture_video_{video_count}_{timestamp}.mp4\"\n",
    "                    self.current_video_path = os.path.join(self.video_dir, video_filename)\n",
    "                    \n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                    self.video_writer = cv2.VideoWriter(\n",
    "                        self.current_video_path, fourcc, fps, \n",
    "                        (processed_frame.shape[1], processed_frame.shape[0])\n",
    "                    )\n",
    "                    \n",
    "                    if self.video_writer.isOpened():\n",
    "                        self.recording = True\n",
    "                        self.recording_start_time = time.time()\n",
    "                        print(f\"üî¥ Started recording: {video_filename}\")\n",
    "        \n",
    "        if self.recording and self.video_writer:\n",
    "            self.video_writer.release()\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def extract_features_from_video(self, video_path, label):\n",
    "        \"\"\"Extract hand features from video file\"\"\"\n",
    "        print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"‚ùå Error: Could not open video {video_path}\")\n",
    "            return [], []\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        frame_count = 0\n",
    "        valid_frames = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % 2 != 0:  # Process every other frame\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                features, _, hands_detected, hand_count = self.extract_hand_features(frame)\n",
    "                \n",
    "                if hands_detected and hand_count > 0:\n",
    "                    features_list.append(features)\n",
    "                    labels_list.append(label)\n",
    "                    valid_frames += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        cap.release()\n",
    "        print(f\"‚úÖ Extracted {valid_frames} valid frames\")\n",
    "        return features_list, labels_list\n",
    "\n",
    "    def train_from_videos(self):\n",
    "        \"\"\"Train model from recorded videos\"\"\"\n",
    "        print(\"\\n== TRAIN FROM VIDEOS ==\")\n",
    "        \n",
    "        # Get all video files\n",
    "        video_files = []\n",
    "        for file in os.listdir(self.video_dir):\n",
    "            if file.endswith(('.mp4', '.avi', '.mov')):\n",
    "                video_files.append(os.path.join(self.video_dir, file))\n",
    "        \n",
    "        if not video_files:\n",
    "            print(\"No video files found in gesture_videos directory.\")\n",
    "            return\n",
    "        \n",
    "        # Label videos\n",
    "        labeled_videos = []\n",
    "        domains = list(self.domain_signs.keys())\n",
    "        \n",
    "        print(f\"Found {len(video_files)} videos\")\n",
    "        print(\"Available domains:\")\n",
    "        for i, domain in enumerate(domains):\n",
    "            print(f\"{i+1}: {self.domain_signs[domain]}\")\n",
    "        \n",
    "        for video_path in video_files:\n",
    "            video_name = os.path.basename(video_path)\n",
    "            print(f\"\\nVideo: {video_name}\")\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    label_choice = input(f\"Enter domain number (1-{len(domains)}) or 'skip': \").strip()\n",
    "                    \n",
    "                    if label_choice.lower() == 'skip':\n",
    "                        break\n",
    "                    \n",
    "                    domain_idx = int(label_choice) - 1\n",
    "                    if 0 <= domain_idx < len(domains):\n",
    "                        domain = domains[domain_idx]\n",
    "                        labeled_videos.append((video_path, domain))\n",
    "                        print(f\"‚úÖ {video_name} -> {self.domain_signs[domain]}\")\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid choice.\")\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input.\")\n",
    "        \n",
    "        if not labeled_videos:\n",
    "            print(\"No labeled videos available.\")\n",
    "            return\n",
    "        \n",
    "        # Extract features\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for video_path, label in labeled_videos:\n",
    "            features, labels = self.extract_features_from_video(video_path, label)\n",
    "            all_features.extend(features)\n",
    "            all_labels.extend(labels)\n",
    "        \n",
    "        if not all_features:\n",
    "            print(\"No features extracted.\")\n",
    "            return\n",
    "        \n",
    "        X = np.array(all_features)\n",
    "        y = np.array(all_labels)\n",
    "        \n",
    "        print(f\"Total training samples: {len(X)}\")\n",
    "        \n",
    "        # Train model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Model accuracy: {accuracy:.2f}\")\n",
    "        \n",
    "        # Save model\n",
    "        with open(self.model_path, 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "        print(f\"‚úÖ Model saved to {self.model_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load trained model\"\"\"\n",
    "        try:\n",
    "            with open(self.model_path, 'rb') as f:\n",
    "                self.model = pickle.load(f)\n",
    "            print(f\"‚úÖ Loaded model from {self.model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not load model: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "    def start_detection(self):\n",
    "        \"\"\"Start real-time domain expansion detection\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"‚ùå No model loaded. Please train a model first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n== DOMAIN EXPANSION DETECTION ==\")\n",
    "        print(\"üî• Starting real-time detection\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"‚ùå Error: Could not open camera.\")\n",
    "            return\n",
    "            \n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        cv2.namedWindow(\"Jujutsu Kaisen Domain Detection\", cv2.WINDOW_NORMAL)\n",
    "        \n",
    "        # Reset detection state\n",
    "        self.gesture_buffer = []\n",
    "        self.stable_gesture = None\n",
    "        self.gesture_confirmed = False\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frame = cv2.flip(frame, 1)\n",
    "            current_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Normal detection only\n",
    "                features, frame, hands_detected, hand_count = self.extract_hand_features(frame)\n",
    "                \n",
    "                if hands_detected and hand_count > 0:\n",
    "                    if current_time - self.prediction_time > self.detection_cooldown:\n",
    "                        prediction = self.model.predict([features])[0]\n",
    "                        probs = self.model.predict_proba([features])[0]\n",
    "                        confidence = np.max(probs)\n",
    "                        \n",
    "                        confirmed_domain, confirmed_confidence = self.update_gesture_buffer(prediction, confidence)\n",
    "                        \n",
    "                        if confirmed_domain:\n",
    "                            print(f\"üî• DOMAIN DETECTED: {self.domain_signs[confirmed_domain]} (confidence: {confirmed_confidence:.2f})\")\n",
    "                            self.last_prediction = confirmed_domain\n",
    "                            self.prediction_confidence = confirmed_confidence\n",
    "                            self.prediction_time = current_time\n",
    "                            \n",
    "                            # Reset detection state\n",
    "                            self.gesture_buffer = []\n",
    "                            self.stable_gesture = None\n",
    "                            self.gesture_confirmed = False\n",
    "                    \n",
    "                    # Show preview when detecting\n",
    "                    if self.stable_gesture and not self.gesture_confirmed:\n",
    "                        time_held = current_time - self.gesture_start_time\n",
    "                        progress = min(1.0, time_held / self.min_detection_time)\n",
    "                        \n",
    "                        # Show character preview in upper right\n",
    "                        avg_conf = sum(e['confidence'] for e in self.gesture_buffer if e['prediction'] == self.stable_gesture) / max(1, sum(1 for e in self.gesture_buffer if e['prediction'] == self.stable_gesture))\n",
    "                        self.show_domain_preview(frame, self.stable_gesture, avg_conf)\n",
    "                        \n",
    "                        # Progress bar\n",
    "                        bar_width = 200\n",
    "                        bar_x = (frame.shape[1] - bar_width) // 2\n",
    "                        bar_y = 50\n",
    "                        \n",
    "                        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + 10), (100, 100, 100), -1)\n",
    "                        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + int(bar_width * progress), bar_y + 10), (0, 255, 0), -1)\n",
    "                        cv2.putText(frame, f\"gesture: {progress*100:.0f}%\", \n",
    "                                   (bar_x, bar_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                \n",
    "                # Status display\n",
    "                if hands_detected:\n",
    "                    cv2.putText(frame, f\"üëã Hands Detected ({hand_count}) - Ready for detection\", \n",
    "                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Show last detection\n",
    "                    if self.last_prediction and current_time - self.prediction_time < 10:\n",
    "                        detection_text = f\"Last: {self.domain_signs[self.last_prediction]} ({self.prediction_confidence:.2f})\"\n",
    "                        cv2.putText(frame, detection_text, (10, 60), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"‚ùå Show hands for detection\", \n",
    "                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "                    self.gesture_buffer = []\n",
    "                    self.stable_gesture = None\n",
    "                    self.gesture_confirmed = False\n",
    "                \n",
    "                # Instructions\n",
    "                cv2.putText(frame, \"Hold gesture steady for detection | Press 'Q' to quit\", \n",
    "                           (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                \n",
    "                cv2.imshow(\"Jujutsu Kaisen Domain Detection\", frame)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Detection error: {e}\")\n",
    "                cv2.putText(frame, f\"Error: {str(e)[:30]}...\", \n",
    "                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "                cv2.imshow(\"Jujutsu Kaisen Domain Detection\", frame)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main menu\"\"\"\n",
    "        while True:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"üî• JUJUTSU KAISEN DOMAIN DETECTION üî•\")\n",
    "            print(\"=\"*50)\n",
    "            print(\"1: üìπ Record Gesture Videos\")\n",
    "            print(\"2: üìä Train Model from Videos\") \n",
    "            print(\"3: üéØ Start Domain Detection\")\n",
    "            print(\"4: üö™ Exit\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            try:\n",
    "                choice = input(\"Select option (1-4): \").strip()\n",
    "                \n",
    "                if choice == '1':\n",
    "                    self.record_gesture_video()\n",
    "                elif choice == '2':\n",
    "                    self.train_from_videos()\n",
    "                elif choice == '3':\n",
    "                    self.start_detection()\n",
    "                elif choice == '4':\n",
    "                    print(\"üëã Exiting...\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"‚ùå Invalid choice.\")\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nüëã Exiting...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"üî• Starting Jujutsu Kaisen Domain Detection System...\")\n",
    "        jjk = JujutsuKaisenGestureRecognition()\n",
    "        jjk.run()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fatal error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecca1a3-c106-4a95-921d-bcb25eeede73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
